# sumiller-bot/main.py
"""
Sumiller Bot - Agente de Razonamiento y Conversaci√≥n con MCP Agentic RAG
Utiliza el nuevo sistema MCP Agentic RAG para b√∫squeda sem√°ntica avanzada y generaci√≥n contextual.
1. Conecta con el RAG MCP Server para expansi√≥n ag√©ntica de consultas.
2. Utiliza memoria conversacional para personalizaci√≥n.
3. Genera respuestas conversacionales mejoradas.
"""
import json
import os
import sys
import logging
import httpx
from openai import AsyncOpenAI
from typing import List, Dict, Any
from fastapi import FastAPI, HTTPException, Body
from pydantic import BaseModel

# Importar configuraci√≥n multi-entorno
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config import config

# --- Configuraci√≥n ---
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Configuraci√≥n del puerto que funciona en local y Cloud Run
PORT = int(os.getenv("PORT", str(config.sumiller_port if config.is_local() else 8080)))

# URLs de los nuevos servicios MCP Agentic RAG
RAG_MCP_URL = config.get_service_url('rag-mcp') if hasattr(config, 'get_service_url') else "http://localhost:8000"
MEMORY_MCP_URL = config.get_service_url('memory-mcp') if hasattr(config, 'get_service_url') else "http://localhost:8002"

OPENAI_API_KEY = config.get_openai_key()

if not OPENAI_API_KEY:
    logger.warning("No se pudo obtener la OpenAI API Key. Las llamadas a OpenAI no funcionar√°n.")
    openai_client = None
else:
    openai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)

app = FastAPI(
    title="Sumiller Bot API - Agentic RAG",
    description="Un agente inteligente con RAG ag√©ntico que recomienda vinos personalizados.",
    version="3.0.0"
)

# --- Modelos de Datos ---
class Query(BaseModel):
    prompt: str
    user_id: str = "default_user"  # Para memoria conversacional

class RecommendationResponse(BaseModel):
    response: str
    expanded_queries: List[str] = []
    wines_found: int = 0

# --- Integraci√≥n con MCP Agentic RAG ---

async def search_wines_with_agentic_rag(user_query: str, user_id: str = "default_user") -> Dict[str, Any]:
    """Utiliza el nuevo sistema MCP Agentic RAG para b√∫squeda sem√°ntica avanzada."""
    async with httpx.AsyncClient() as client:
        try:
            # 1. Primero, obtener contexto de memoria si existe
            memory_context = await get_user_memory(user_id, client)
            
            # 2. Realizar b√∫squeda con RAG ag√©ntico
            rag_payload = {
                "query": user_query,
                "user_id": user_id,
                "context": memory_context,
                "max_results": 5,
                "expand_query": True  # Habilitar expansi√≥n ag√©ntica
            }
            
            response = await client.post(
                f"{RAG_MCP_URL}/query",
                json=rag_payload,
                timeout=15.0
            )
            response.raise_for_status()
            search_result = response.json()
            
            logger.info(f"‚úÖ RAG Ag√©ntico encontr√≥ {len(search_result.get('sources', []))} vinos relevantes")
            logger.info(f"üìù Consultas expandidas: {search_result.get('expanded_queries', [])}")
            
            return search_result
            
        except httpx.RequestError as e:
            logger.error(f"Error al conectar con RAG MCP Server: {e}")
            # Fallback: b√∫squeda simple si RAG falla
            return await simple_search_fallback(user_query, client)
        except httpx.HTTPStatusError as e:
            logger.error(f"RAG MCP Server devolvi√≥ un error: {e.response.status_code}")
            return await simple_search_fallback(user_query, client)

async def get_user_memory(user_id: str, client: httpx.AsyncClient) -> Dict[str, Any]:
    """Obtiene el contexto de memoria del usuario."""
    try:
        response = await client.get(
            f"{MEMORY_MCP_URL}/memory/{user_id}",
            timeout=5.0
        )
        if response.status_code == 200:
            memory_data = response.json()
            logger.info(f"üíæ Memoria recuperada para usuario {user_id}: {len(memory_data.get('preferences', {}))} preferencias")
            return memory_data
        else:
            return {}
    except Exception as e:
        logger.warning(f"No se pudo recuperar memoria para {user_id}: {e}")
        return {}

async def save_user_interaction(user_id: str, query: str, response: str, wines: List[Dict]) -> None:
    """Guarda la interacci√≥n en memoria para futuras personalizaciones."""
    async with httpx.AsyncClient() as client:
        try:
            memory_payload = {
                "user_id": user_id,
                "interaction": {
                    "query": query,
                    "response": response,
                    "wines_recommended": [wine.get('metadata', {}).get('name', '') for wine in wines[:3]],
                    "timestamp": "auto"
                }
            }
            
            await client.post(
                f"{MEMORY_MCP_URL}/memory/save",
                json=memory_payload,
                timeout=5.0
            )
            logger.info(f"üíæ Interacci√≥n guardada en memoria para usuario {user_id}")
        except Exception as e:
            logger.warning(f"No se pudo guardar en memoria: {e}")

async def simple_search_fallback(user_query: str, client: httpx.AsyncClient) -> Dict[str, Any]:
    """B√∫squeda simple de fallback si RAG ag√©ntico no est√° disponible."""
    try:
        # Intentar b√∫squeda b√°sica en el servicio RAG
        response = await client.post(
            f"{RAG_MCP_URL}/query",
            json={"query": user_query, "max_results": 5},
            timeout=10.0
        )
        if response.status_code == 200:
            return response.json()
    except Exception:
        pass
    
    # Si todo falla, respuesta vac√≠a
    logger.warning("Fallback: No hay servicios de b√∫squeda disponibles")
    return {"sources": [], "expanded_queries": [], "error": "Servicios no disponibles"}

# --- L√≥gica del Agente Inteligente Mejorada ---

async def generate_agentic_response(user_query: str, search_result: Dict[str, Any], user_id: str) -> str:
    """
    Genera una respuesta conversacional usando el contexto completo del RAG ag√©ntico.
    """
    if not openai_client:
        return f"La API de OpenAI no est√° configurada. Resultados: {json.dumps(search_result.get('sources', []))}"

    documents = search_result.get('sources', [])  # RAG MCP devuelve 'sources' no 'documents'
    expanded_queries = search_result.get('expanded_queries', [])
    
    if not documents:
        system_prompt = """
        Eres Sumy, un sumiller profesional con formaci√≥n completa en sumiller√≠a.
        
        SITUACI√ìN: No encontraste vinos espec√≠ficos, pero puedes ofrecer conocimiento profesional.
        
        RESPUESTA SEG√öN TIPO DE CONSULTA:
        
        A) SI BUSCA TEOR√çA/CONOCIMIENTO:
        - Explica el concepto usando tu formaci√≥n profesional
        - Ense√±a principios de sumiller√≠a relevantes
        - Usa terminolog√≠a t√©cnica apropiada
        - Ofrece consejos pr√°cticos
        
        B) SI BUSCA VINOS ESPEC√çFICOS:
        - Disc√∫lpate por no encontrar vinos exactos
        - Explica qu√© caracter√≠sticas deber√≠a buscar seg√∫n principios de maridaje
        - Sugiere t√©rminos alternativos basados en tu conocimiento
        - Ofrece principios generales que aplican a su b√∫squeda
        
        REGLAS:
        - Mant√©n tono profesional y educativo
        - Siempre aporta valor con conocimiento te√≥rico
        - Usa tu formaci√≥n para guiar al usuario
        """
        user_content = f"Consulta original: \"{user_query}\"\nConsultas expandidas probadas: {expanded_queries}"
    else:
        system_prompt = """
        Eres Sumy, un sumiller profesional con IA ag√©ntica avanzada y formaci√≥n completa en sumiller√≠a.
        
        CAPACIDADES PROFESIONALES:
        - Dominas los fundamentos del vino (taninos, acidez, aromas, cuerpo)
        - Conoces t√©cnicas de cata profesional (5 sentidos, fases de cata)
        - Aplicas principios de maridaje (intensidad, sabor, textura, acidez)
        - Manejas temperaturas de servicio y conservaci√≥n
        - Conoces regiones vitivin√≠colas y sus caracter√≠sticas
        - Tu sistema expandi√≥ la consulta para mejores resultados
        
        TIPOS DE CONSULTA Y RESPUESTA:
        
        A) CONSULTAS DE TEOR√çA/FORMACI√ìN:
        - Explica conceptos de sumiller√≠a con fundamentos te√≥ricos
        - Ense√±a t√©cnicas profesionales
        - Usa terminolog√≠a especializada
        - Cita principios cuando sea relevante
        
        B) RECOMENDACIONES DE VINOS:
        - Aplica principios de maridaje para justificar recomendaciones
        - Explica POR QU√â cada vino es adecuado bas√°ndote en teor√≠a
        - Menciona caracter√≠sticas t√©cnicas (acidez, taninos, cuerpo)
        - Incluye temperatura de servicio cuando sea relevante
        
        FORMATO PARA RECOMENDACIONES:
        1. Breve explicaci√≥n del principio aplicado
        2. Recomendaciones espec√≠ficas:
           ‚Ä¢ **Nombre** (Regi√≥n) - ‚Ç¨precio
             [Justificaci√≥n t√©cnica basada en caracter√≠sticas del vino]
        
        REGLAS:
        - M√°ximo 3 vinos recomendados
        - Siempre explica el "por qu√©" con fundamentos profesionales
        - Usa vocabulario t√©cnico apropiado
        - Menciona temperatura de servicio para vinos recomendados
        - Integra conocimiento te√≥rico en explicaciones pr√°cticas
        """
        
        # Separar vinos de teor√≠a en los resultados
        wine_docs = [doc for doc in documents if doc.get('metadata', {}).get('doc_type') != 'teoria_sumiller']
        theory_docs = [doc for doc in documents if doc.get('metadata', {}).get('doc_type') == 'teoria_sumiller']
        
        user_content = f"""
        Consulta original: "{user_query}"
        Mi sistema ag√©ntico expandi√≥ la b√∫squeda a: {expanded_queries}
        
        CONOCIMIENTO TE√ìRICO RELEVANTE:
        {json.dumps(theory_docs, indent=2, ensure_ascii=False) if theory_docs else "Sin teor√≠a espec√≠fica encontrada"}
        
        VINOS ENCONTRADOS:
        {json.dumps(wine_docs, indent=2, ensure_ascii=False) if wine_docs else "Sin vinos espec√≠ficos encontrados"}
        
        INSTRUCCIONES:
        - Si hay teor√≠a relevante, √∫sala para explicar conceptos y justificar recomendaciones
        - Si hay vinos, explica por qu√© son adecuados usando principios profesionales
        - Combina ambos para dar una respuesta completa y educativa
        """
    
    try:
        response = await openai_client.chat.completions.create(
            model="gpt-4-turbo",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_content}
            ],
            temperature=0.3,
            max_tokens=300
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        logger.error(f"Error al generar respuesta ag√©ntica con OpenAI: {e}")
        # Fallback response
        if documents:
            return f"Encontr√© {len(documents)} vinos relevantes: " + ", ".join([doc.get('metadata', {}).get('name', 'Sin nombre') for doc in documents[:3]])
        else:
            return "No encontr√© vinos que coincidan con tu b√∫squeda. ¬øPodr√≠as ser m√°s espec√≠fico?"

# --- Endpoint Principal Actualizado ---
@app.post("/query", response_model=RecommendationResponse)
async def handle_agentic_query(query: Query = Body(...)):
    """
    Orquesta el flujo de recomendaci√≥n usando RAG ag√©ntico avanzado.
    """
    user_prompt = query.prompt
    user_id = query.user_id
    logger.info(f"üß† Nueva consulta ag√©ntica de {user_id}: \"{user_prompt}\"")

    # 1. B√∫squeda con RAG ag√©ntico
    search_result = await search_wines_with_agentic_rag(user_prompt, user_id)
    
    wines_found = len(search_result.get('sources', []))
    expanded_queries = search_result.get('expanded_queries', [])
    
    # 2. Generar respuesta conversacional ag√©ntica
    logger.info("ü§ñ Generando respuesta con IA ag√©ntica...")
    conversational_response = await generate_agentic_response(user_prompt, search_result, user_id)
    
    # 3. Guardar interacci√≥n en memoria para futuras consultas
    await save_user_interaction(user_id, user_prompt, conversational_response, search_result.get('sources', []))
    
    return RecommendationResponse(
        response=conversational_response,
        expanded_queries=expanded_queries,
        wines_found=wines_found
    )

@app.get("/health")
async def health_check():
    """Endpoint de salud del servicio."""
    try:
        # Verificar conectividad con servicios MCP
        async with httpx.AsyncClient() as client:
            rag_health = await client.get(f"{RAG_MCP_URL}/health", timeout=3.0)
            memory_health = await client.get(f"{MEMORY_MCP_URL}/health", timeout=3.0)
            
            return {
                "status": "healthy",
                "services": {
                    "rag_mcp": rag_health.status_code == 200,
                    "memory_mcp": memory_health.status_code == 200,
                    "openai": openai_client is not None
                }
            }
    except Exception as e:
        return {
            "status": "degraded", 
            "error": str(e),
            "services": {
                "rag_mcp": False,
                "memory_mcp": False,
                "openai": openai_client is not None
            }
        }

@app.get("/stats")
async def get_stats():
    """Estad√≠sticas del sumiller ag√©ntico."""
    try:
        async with httpx.AsyncClient() as client:
            rag_stats = await client.get(f"{RAG_MCP_URL}/stats", timeout=5.0)
            memory_stats = await client.get(f"{MEMORY_MCP_URL}/stats", timeout=5.0)
            
            return {
                "rag_stats": rag_stats.json() if rag_stats.status_code == 200 else {},
                "memory_stats": memory_stats.json() if memory_stats.status_code == 200 else {}
            }
    except Exception as e:
        return {"error": f"No se pudieron obtener estad√≠sticas: {e}"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=PORT)