# sumiller-bot/main.py
"""
Sumiller Bot - Agente de Razonamiento y Conversaci√≥n con MCP Agentic RAG
Utiliza el nuevo sistema MCP Agentic RAG para b√∫squeda sem√°ntica avanzada y generaci√≥n contextual.
1. Conecta con el RAG MCP Server para expansi√≥n ag√©ntica de consultas.
2. Utiliza memoria conversacional para personalizaci√≥n.
3. Genera respuestas conversacionales mejoradas.
"""
import os
import sys
import logging
import httpx
import json
from openai import AsyncOpenAI
from typing import List, Dict, Any
from fastapi import FastAPI, HTTPException, Body
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

# Importar configuraci√≥n multi-entorno
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config import config

# --- Configuraci√≥n ---
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Configuraci√≥n del puerto que funciona en local y Cloud Run
PORT = int(os.getenv("PORT", str(config.sumiller_port if config.is_local() else 8080)))

# URLs de los nuevos servicios MCP Agentic RAG
RAG_MCP_URL = os.getenv('RAG_MCP_URL', "http://localhost:8000")
MEMORY_MCP_URL = os.getenv('MEMORY_MCP_URL', "http://localhost:8002")

# Configuraci√≥n DeepSeek
DEEPSEEK_API_KEY = config.get_deepseek_key()
DEEPSEEK_BASE_URL = config.get_deepseek_base_url()
DEEPSEEK_MODEL = config.get_deepseek_model()

if not DEEPSEEK_API_KEY:
    logger.warning("No se pudo obtener la DeepSeek API Key. Las llamadas a DeepSeek no funcionar√°n.")
    deepseek_client = None
else:
    # Usar el cliente OpenAI pero con la configuraci√≥n de DeepSeek
    deepseek_client = AsyncOpenAI(
        api_key=DEEPSEEK_API_KEY,
        base_url=DEEPSEEK_BASE_URL
    )
    logger.info(f"‚úÖ Cliente DeepSeek configurado: {DEEPSEEK_BASE_URL} - Modelo: {DEEPSEEK_MODEL}")

app = FastAPI(
    title="Sumiller Bot API - Agentic RAG",
    description="Un agente inteligente con RAG ag√©ntico que recomienda vinos personalizados.",
    version="3.0.0"
)

# Configuraci√≥n de CORS para permitir conexiones desde la UI
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://127.0.0.1:3000"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Modelos de Datos ---
class Query(BaseModel):
    prompt: str
    user_id: str = "default_user"  # Para memoria conversacional

class RecommendationResponse(BaseModel):
    response: str
    expanded_queries: List[str] = []
    wines_found: int = 0

# --- Integraci√≥n con MCP Agentic RAG ---

async def search_wines_with_agentic_rag(user_query: str, user_id: str = "default_user") -> Dict[str, Any]:
    """Utiliza el nuevo sistema MCP Agentic RAG para b√∫squeda sem√°ntica avanzada."""
    logger.info(f"üîç Iniciando b√∫squeda RAG para: '{user_query}'")
    logger.info(f"üîó URL del RAG MCP: {RAG_MCP_URL}")
    
    async with httpx.AsyncClient() as client:
        try:
            # 1. Primero, obtener contexto de memoria si existe
            logger.info("üíæ Obteniendo contexto de memoria...")
            memory_context = await get_user_memory(user_id, client)
            logger.info(f"üíæ Contexto de memoria obtenido: {bool(memory_context)}")
            
            # 2. Realizar b√∫squeda con RAG ag√©ntico
            rag_payload = {
                "query": user_query,
                "max_results": 3  # Reducido para menor latencia
            }
            
            # Solo agregar contexto si existe para reducir payload
            if memory_context:
                rag_payload["context"] = memory_context
            
            logger.info(f"üì§ Enviando payload a {RAG_MCP_URL}/query")
            logger.info(f"üì¶ Payload: {rag_payload}")
            
            response = await client.post(
                f"{RAG_MCP_URL}/query",
                json=rag_payload,
                timeout=30.0  # Aumentado para b√∫squedas complejas
            )
            
            logger.info(f"üì° Respuesta recibida: status={response.status_code}")
            response.raise_for_status()
            search_result = response.json()
            
            logger.info(f"‚úÖ RAG Ag√©ntico encontr√≥ {len(search_result.get('sources', []))} vinos relevantes")
            logger.info(f"üìù Consultas expandidas: {search_result.get('expanded_queries', [])}")
            
            return search_result
            
        except httpx.RequestError as e:
            logger.error(f"Error al conectar con RAG MCP Server: {e}")
            logger.error(f"URL intentada: {RAG_MCP_URL}/query")
            # Fallback: b√∫squeda simple si RAG falla
            return await simple_search_fallback(user_query, client)
        except httpx.HTTPStatusError as e:
            logger.error(f"RAG MCP Server devolvi√≥ un error: {e.response.status_code}")
            logger.error(f"Respuesta del servidor: {e.response.text}")
            return await simple_search_fallback(user_query, client)
        except Exception as e:
            logger.error(f"Error inesperado en RAG MCP: {e}")
            logger.error(f"Tipo de error: {type(e)}")
            return await simple_search_fallback(user_query, client)

async def get_user_memory(user_id: str, client: httpx.AsyncClient) -> Dict[str, Any]:
    """Obtiene el contexto de memoria del usuario."""
    try:
        response = await client.get(
            f"{MEMORY_MCP_URL}/memory/{user_id}",
            timeout=5.0
        )
        if response.status_code == 200:
            memory_data = response.json()
            logger.info(f"üíæ Memoria recuperada para usuario {user_id}: {len(memory_data.get('preferences', {}))} preferencias")
            return memory_data
        else:
            return {}
    except Exception as e:
        logger.warning(f"No se pudo recuperar memoria para {user_id}: {e}")
        return {}

async def save_user_interaction(user_id: str, query: str, response: str, wines: List[Dict]) -> None:
    """Guarda la interacci√≥n en memoria para futuras personalizaciones."""
    async with httpx.AsyncClient() as client:
        try:
            memory_payload = {
                "user_id": user_id,
                "interaction": {
                    "query": query,
                    "response": response,
                    "wines_recommended": [wine.get('metadata', {}).get('name', '') for wine in wines[:3]],
                    "timestamp": "auto"
                }
            }
            
            await client.post(
                f"{MEMORY_MCP_URL}/memory/save",
                json=memory_payload,
                timeout=5.0
            )
            logger.info(f"üíæ Interacci√≥n guardada en memoria para usuario {user_id}")
        except Exception as e:
            logger.warning(f"No se pudo guardar en memoria: {e}")

async def simple_search_fallback(user_query: str, client: httpx.AsyncClient) -> Dict[str, Any]:
    """B√∫squeda simple de fallback si RAG ag√©ntico no est√° disponible."""
    try:
        # Intentar b√∫squeda b√°sica en el servicio RAG
        logger.info(f"üîÑ Intentando fallback con URL: {RAG_MCP_URL}/query")
        response = await client.post(
            f"{RAG_MCP_URL}/query",
            json={"query": user_query, "max_results": 3},
            timeout=20.0  # Timeout m√°s generoso para fallback
        )
        logger.info(f"üì° Respuesta fallback status: {response.status_code}")
        if response.status_code == 200:
            result = response.json()
            logger.info(f"‚úÖ Fallback exitoso: {len(result.get('sources', []))} fuentes encontradas")
            return result
    except Exception as e:
        logger.error(f"‚ùå Error en fallback: {e}")
        logger.error(f"üîç Tipo de error en fallback: {type(e)}")
    
    # Si todo falla, respuesta vac√≠a
    logger.warning("Fallback: No hay servicios de b√∫squeda disponibles")
    return {"sources": [], "expanded_queries": [], "error": "Servicios no disponibles"}

# --- L√≥gica del Agente Inteligente Mejorada ---

async def generate_agentic_response(user_query: str, search_result: Dict[str, Any], user_id: str) -> str:
    """
    Genera una respuesta conversacional usando el contexto completo del RAG ag√©ntico.
    """
    if not deepseek_client:
        return f"La API de DeepSeek no est√° configurada. Resultados: {json.dumps(search_result.get('sources', []))}"

    documents = search_result.get('sources', [])  # RAG MCP devuelve 'sources' no 'documents'
    expanded_queries = search_result.get('expanded_queries', [])
    
    if not documents:
        system_prompt = """
        Eres Sumy, un sumiller profesional con formaci√≥n completa en sumiller√≠a.
        
        SITUACI√ìN: No encontraste vinos espec√≠ficos, pero puedes ofrecer conocimiento profesional.
        
        RESPUESTA SEG√öN TIPO DE CONSULTA:
        
        A) SI BUSCA TEOR√çA/CONOCIMIENTO:
        - Explica el concepto usando tu formaci√≥n profesional
        - Ense√±a principios de sumiller√≠a relevantes
        - Usa terminolog√≠a t√©cnica apropiada
        - Ofrece consejos pr√°cticos
        
        B) SI BUSCA VINOS ESPEC√çFICOS:
        - Disc√∫lpate por no encontrar vinos exactos
        - Explica qu√© caracter√≠sticas deber√≠a buscar seg√∫n principios de maridaje
        - Sugiere t√©rminos alternativos basados en tu conocimiento
        - Ofrece principios generales que aplican a su b√∫squeda
        
        REGLAS:
        - Mant√©n tono profesional y educativo
        - Siempre aporta valor con conocimiento te√≥rico
        - Usa tu formaci√≥n para guiar al usuario
        """
        user_content = f"Consulta original: \"{user_query}\"\nConsultas expandidas probadas: {expanded_queries}"
    else:
        system_prompt = """
        Eres Sumy, un sumiller profesional con IA ag√©ntica avanzada y formaci√≥n completa en sumiller√≠a.
        
        CAPACIDADES PROFESIONALES:
        - Dominas los fundamentos del vino (taninos, acidez, aromas, cuerpo)
        - Conoces t√©cnicas de cata profesional (5 sentidos, fases de cata)
        - Aplicas principios de maridaje (intensidad, sabor, textura, acidez)
        - Manejas temperaturas de servicio y conservaci√≥n
        - Conoces regiones vitivin√≠colas y sus caracter√≠sticas
        - Tu sistema expandi√≥ la consulta para mejores resultados
        
        TIPOS DE CONSULTA Y RESPUESTA:
        
        A) SALUDOS Y CONVERSACI√ìN GENERAL:
        Si la consulta es un saludo, pregunta general, o no especifica vinos:
        - Responde de forma amable y profesional
        - Pres√©ntate brevemente como Sumy, el sumiller virtual
        - Indica en qu√© puedes ayudar (recomendaciones, maridajes, teor√≠a)
        - NO uses marcadores de estructura
        
        B) CONSULTAS DE TEOR√çA/FORMACI√ìN:
        [PRINCIPIO]
        Explica conceptos de sumiller√≠a con fundamentos te√≥ricos completos
        [/PRINCIPIO]
        
        C) RECOMENDACIONES DE VINOS:
        [PRINCIPIO]
        Explica brevemente el principio o fundamento aplicado (m√°ximo 2 l√≠neas)
        [/PRINCIPIO]

        [RECOMENDACION_1]
        **Nombre del Vino** (Regi√≥n) - ‚Ç¨precio
        Justificaci√≥n t√©cnica concisa basada en caracter√≠sticas espec√≠ficas
        Temperatura de servicio: X¬∞C
        [/RECOMENDACION_1]

        REGLAS ESTRICTAS:
        - Para recomendaciones: SOLO 1 vino (el mejor)
        - Para teor√≠a: usa solo [PRINCIPIO]
        - Para saludos: respuesta natural sin marcadores
        - Cada recomendaci√≥n debe ser concisa (2-3 l√≠neas m√°ximo)
        - SIEMPRE incluye temperatura de servicio en recomendaciones
        """
        
        # Separar vinos de teor√≠a en los resultados
        wine_docs = [doc for doc in documents if doc.get('metadata', {}).get('doc_type') != 'teoria_sumiller']
        theory_docs = [doc for doc in documents if doc.get('metadata', {}).get('doc_type') == 'teoria_sumiller']
        
        user_content = f"""
        Consulta original: "{user_query}"
        Mi sistema ag√©ntico expandi√≥ la b√∫squeda a: {expanded_queries}
        
        CONOCIMIENTO TE√ìRICO RELEVANTE:
        {json.dumps(theory_docs, indent=2, ensure_ascii=False) if theory_docs else "Sin teor√≠a espec√≠fica encontrada"}
        
        VINOS ENCONTRADOS:
        {json.dumps(wine_docs, indent=2, ensure_ascii=False) if wine_docs else "Sin vinos espec√≠ficos encontrados"}
        
        INSTRUCCIONES:
        - Si hay teor√≠a relevante, √∫sala para explicar conceptos y justificar recomendaciones
        - Si hay vinos, explica por qu√© son adecuados usando principios profesionales
        - Combina ambos para dar una respuesta completa y educativa
        """
    
    try:
        response = await deepseek_client.chat.completions.create(
            model=DEEPSEEK_MODEL,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_content}
            ],
            temperature=0.3,
            max_tokens=300
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        logger.error(f"Error al generar respuesta ag√©ntica con DeepSeek: {e}")
        # Fallback response
        if documents:
            return f"Encontr√© {len(documents)} vinos relevantes: " + ", ".join([doc.get('metadata', {}).get('name', 'Sin nombre') for doc in documents[:3]])
        else:
            return "No encontr√© vinos que coincidan con tu b√∫squeda. ¬øPodr√≠as ser m√°s espec√≠fico?"

# --- Endpoint Principal Actualizado ---
@app.post("/query", response_model=RecommendationResponse)
async def handle_agentic_query(query: Query = Body(...)):
    """
    Orquesta el flujo de recomendaci√≥n usando RAG ag√©ntico avanzado.
    """
    user_prompt = query.prompt
    user_id = query.user_id
    logger.info(f"üß† Nueva consulta ag√©ntica de {user_id}: \"{user_prompt}\"")

    # 1. B√∫squeda con RAG ag√©ntico
    try:
        logger.info("üîÑ Llamando a search_wines_with_agentic_rag...")
        search_result = await search_wines_with_agentic_rag(user_prompt, user_id)
        logger.info("‚úÖ search_wines_with_agentic_rag completado")
    except Exception as e:
        logger.error(f"üí• Error en search_wines_with_agentic_rag: {e}")
        logger.error(f"üîç Tipo de error: {type(e)}")
        # Fallback directo
        search_result = {"sources": [], "expanded_queries": [], "error": str(e)}
    
    wines_found = len(search_result.get('sources', []))
    expanded_queries = search_result.get('expanded_queries', [])
    
    # 2. Generar respuesta conversacional ag√©ntica
    logger.info("ü§ñ Generando respuesta con IA ag√©ntica...")
    conversational_response = await generate_agentic_response(user_prompt, search_result, user_id)
    
    # 3. Guardar interacci√≥n en memoria para futuras consultas
    await save_user_interaction(user_id, user_prompt, conversational_response, search_result.get('sources', []))
    
    return RecommendationResponse(
        response=conversational_response,
        expanded_queries=expanded_queries,
        wines_found=wines_found
    )

@app.get("/health")
async def health_check():
    """Endpoint de salud del servicio."""
    try:
        # Verificar conectividad con servicios MCP
        async with httpx.AsyncClient() as client:
            rag_health = await client.get(f"{RAG_MCP_URL}/health", timeout=3.0)
            memory_health = await client.get(f"{MEMORY_MCP_URL}/health", timeout=3.0)
            
            return {
                "status": "healthy",
                "services": {
                    "rag_mcp": rag_health.status_code == 200,
                    "memory_mcp": memory_health.status_code == 200,
                    "deepseek": deepseek_client is not None
                }
            }
    except Exception as e:
        return {
            "status": "degraded", 
            "error": str(e),
            "services": {
                "rag_mcp": False,
                "memory_mcp": False,
                "deepseek": deepseek_client is not None
            }
        }

@app.get("/stats")
async def get_stats():
    """Estad√≠sticas del sumiller ag√©ntico."""
    try:
        async with httpx.AsyncClient() as client:
            rag_stats = await client.get(f"{RAG_MCP_URL}/stats", timeout=5.0)
            memory_stats = await client.get(f"{MEMORY_MCP_URL}/stats", timeout=5.0)
            
            return {
                "rag_stats": rag_stats.json() if rag_stats.status_code == 200 else {},
                "memory_stats": memory_stats.json() if memory_stats.status_code == 200 else {}
            }
    except Exception as e:
        return {"error": f"No se pudieron obtener estad√≠sticas: {e}"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=PORT)